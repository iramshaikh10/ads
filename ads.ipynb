{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7N1rD6Uz/XJJ8uGWt1RUD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rushiranpise-ltce/ads/blob/main/ads.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Find the data distributions using box and scatter plot for housing dataset"
      ],
      "metadata": {
        "id": "ZKGyfk50At8G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjaBNXOSAmOq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "df = pd.read_csv(\"https://github.com/rushiranpise-ltce/ads/raw/main/Housing.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Box Plot"
      ],
      "metadata": {
        "id": "1FUteZGZB-D1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x=df['price'])\n",
        "plt.show()\n",
        "\n",
        "sns.boxplot(x=df['area'])\n",
        "plt.show()\n",
        "\n",
        "sns.boxplot(x=df['bedrooms'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SGzV-0feBB1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scatter Plot"
      ],
      "metadata": {
        "id": "UJ8UegOIB9ly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(x=df[\"price\"], y=df[\"area\"])\n",
        "plt.show()\n",
        "\n",
        "sns.scatterplot(x=df[\"area\"], y=df[\"bedrooms\"])\n",
        "plt.show()\n",
        "\n",
        "sns.scatterplot(x=df[\"price\"], y=df[\"bedrooms\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GhKmwbdJB7hE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Find the outliers using plot. v=c(135,140,50,75,100,125,150,175,200)"
      ],
      "metadata": {
        "id": "fLTrrpf-Dmg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "v = [135, 140, 50, 75, 100, 125, 150, 175, 200]\n",
        "plt.boxplot(v)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WmMDFWugDt1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Plot the histogram, bar chart and pie chart on any sample data."
      ],
      "metadata": {
        "id": "fwBnuwliDzUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate sample data\n",
        "data = np.random.normal(loc=10, scale=3, size=1000)\n",
        "\n",
        "# Create histogram\n",
        "plt.hist(data, bins=20, edgecolor='black')\n",
        "plt.title('Histogram of Sample Data')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Create bar chart\n",
        "unique, counts = np.unique(data.astype(int), return_counts=True)\n",
        "plt.bar(unique, counts)\n",
        "plt.title('Bar Chart of Sample Data')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "# Create pie chart\n",
        "labels = ['Group A', 'Group B', 'Group C', 'Group D']\n",
        "sizes = [15, 30, 45, 10]\n",
        "plt.pie(sizes, labels=labels, autopct='%1.1f%%')\n",
        "plt.title('Pie Chart of Sample Data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1ejdYx0SD2yD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. How to find a correlation matrix and plot the correlation on iris data set"
      ],
      "metadata": {
        "id": "lhC-K6fnD89F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n",
        "iris_df.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "corr_matrix = iris_df.corr()\n",
        "\n",
        "# Plot the correlation matrix as a heatmap\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='YlGnBu')\n",
        "plt.title('Correlation Matrix of Iris Dataset')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2iaWMFjFEWZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Outlier Distance based method. The example set used in this process is the Iris dataset with four numerical attributes and 150 examples. Find  Top five outliers of Iris dataset when  k=1  and  k =5."
      ],
      "metadata": {
        "id": "4BQJ0nNPEY3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n",
        "iris_df.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
        "\n",
        "# Calculate pairwise Euclidean distances\n",
        "distances = np.zeros((len(iris_df), len(iris_df)))\n",
        "for i in range(len(iris_df)):\n",
        "    for j in range(len(iris_df)):\n",
        "        distances[i][j] = np.sqrt(np.sum((iris_df.iloc[i,:4] - iris_df.iloc[j,:4])**2))\n",
        "\n",
        "# Calculate kth smallest distances\n",
        "k1_distances = np.sort(distances)[:,1]\n",
        "k5_distances = np.sort(distances)[:,5]\n",
        "\n",
        "# Find the top 5 outliers for k=1 and k=5\n",
        "top5_k1 = iris_df.iloc[np.argsort(k1_distances)[-5:],:]\n",
        "top5_k5 = iris_df.iloc[np.argsort(k5_distances)[-5:],:]\n",
        "\n",
        "print(\"Top 5 outliers with k=1:\\n\", top5_k1)\n",
        "print(\"\\nTop 5 outliers with k=5:\\n\", top5_k5)"
      ],
      "metadata": {
        "id": "eSpL8S58Ees9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Consider the following mice data: Height:140,142,150,147,139,152,154,135,148, 147. Weight: 59, 61, 66, 62, 57, 68, 69, 58, 63, 62. Derive relationship coefficients and summary for the above data. Find the evaluation matrices for the the data "
      ],
      "metadata": {
        "id": "eWIAqK5YEr06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "height = [140, 142, 150, 147, 139, 152, 154, 135, 148, 147]\n",
        "weight = [59, 61, 66, 62, 57, 68, 69, 58, 63, 62]\n",
        "mean_height = np.mean(height)\n",
        "mean_weight = np.mean(weight)\n",
        "std_height = np.std(height)\n",
        "std_weight = np.std(weight)\n",
        "\n",
        "print(\"Mean height:\", mean_height)\n",
        "print(\"Mean weight:\", mean_weight)\n",
        "print(\"Standard deviation of height:\", std_height)\n",
        "print(\"Standard deviation of weight:\", std_weight)\n",
        "\n",
        "covariance = np.cov(height, weight)[0][1]\n",
        "print(\"Covariance:\", covariance)\n",
        "corr_coef = covariance / (std_height * std_weight)\n",
        "print(\"Pearson correlation coefficient:\", corr_coef)\n",
        "r_squared = corr_coef**2\n",
        "print(\"Coefficient of determination (r-squared):\", r_squared)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(height, weight)\n",
        "plt.xlabel(\"Height\")\n",
        "plt.ylabel(\"Weight\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y2odp3jOE6DP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "slope, intercept, r_value, p_value, std_err = stats.linregress(height, weight)\n",
        "print(\"Slope:\", slope)\n",
        "print(\"Intercept:\", intercept)\n",
        "print(\"R-squared:\", r_value**2)\n",
        "print(\"Standard error:\", std_err)\n",
        "\n",
        "plt.scatter(height, weight)\n",
        "plt.plot(height, intercept + slope*np.array(height), 'r')\n",
        "plt.xlabel(\"Height\")\n",
        "plt.ylabel(\"Weight\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BQk_O5jqFD2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Write a python code to decompose time series data into random, trend and seasonal data."
      ],
      "metadata": {
        "id": "2WL4uyzwE7fS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas import read_csv\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from matplotlib import pyplot\n",
        "\n",
        "\n",
        "series = read_csv('https://github.com/rushiranpise-ltce/ads/raw/main/airline-passengers.csv', header=0, index_col=0)\n",
        "series.plot()\n",
        "pyplot.show()\n",
        "\n",
        "series = read_csv('https://github.com/rushiranpise-ltce/ads/raw/main/airline-passengers.csv', header=0, index_col=0)\n",
        "result = seasonal_decompose(series, model='multiplicative', period=1)\n",
        "result.plot()\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "HqBpHNykE_Ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. write a python program to calculate the variance for :\n",
        "    sample1 = (1, 2, 5, 4, 8, 9, 12)\n",
        "    sample2 = (-2, -4, -3, -1, -5, -6)\n",
        "    sample3 = (-9, -1, -0, 2, 1, 3, 4, 19)\n",
        "    sample5 = (1.23, 1.45, 2.1, 2.2, 1.9)\n"
      ],
      "metadata": {
        "id": "GiVJllleFfMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "sample1 = (1, 2, 5, 4, 8, 9, 12)\n",
        "sample2 = (-2, -4, -3, -1, -5, -6)\n",
        "sample3 = (-9, -1, 0, 2, 1, 3, 4, 19)\n",
        "sample4 = (1.23, 1.45, 2.1, 2.2, 1.9)\n",
        "\n",
        "variance1 = np.var(sample1, ddof=1)\n",
        "variance2 = np.var(sample2, ddof=1)\n",
        "variance3 = np.var(sample3, ddof=1)\n",
        "variance4 = np.var(sample4, ddof=1)\n",
        "\n",
        "print(\"Variance of sample1: {:.2f}\".format(variance1))\n",
        "print(\"Variance of sample2: {:.2f}\".format(variance2))\n",
        "print(\"Variance of sample3: {:.2f}\".format(variance3))\n",
        "print(\"Variance of sample4: {:.2f}\".format(variance4))"
      ],
      "metadata": {
        "id": "0js1t5ZZFkQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. How do you find outliers in Uber fares dataset using visualization techniques?"
      ],
      "metadata": {
        "id": "GyBXFPYPFosY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_csv('https://github.com/rushiranpise-ltce/ads/raw/main/uber.csv')\n",
        "\n",
        "plt.boxplot(df['fare_amount'])\n",
        "plt.show()\n",
        "\n",
        "plt.hist(df['fare_amount'], bins=30)\n",
        "plt.show()\n",
        "\n",
        "plt.scatter(df['passenger_count'], df['fare_amount'])\n",
        "plt.xlabel('passenger_count')\n",
        "plt.ylabel('fare_amount')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NqNKwryyFu2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Finding outliers using statistical methods for any dataset ?"
      ],
      "metadata": {
        "id": "MIMSbUwJF1i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 100])\n",
        "\n",
        "# Calculate the IQR\n",
        "q1, q3 = np.percentile(data, [25, 75])\n",
        "iqr = q3 - q1\n",
        "\n",
        "# Identify the outliers\n",
        "threshold = 1.5\n",
        "lower_bound = q1 - threshold * iqr\n",
        "upper_bound = q3 + threshold * iqr\n",
        "outliers = np.where((data < lower_bound) | (data > upper_bound))\n",
        "\n",
        "print(\"Outliers:\", data[outliers])"
      ],
      "metadata": {
        "id": "slkpUf8UF4HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. The number of buses that come on a bus stop in span of 30 minutes is 1.Write a program in to represent  Poisson distribution  to be used to model the probability of different number of buses, X, coming to the bus stop within the next 30 minutes where X can take value of 0, 1, 2, 3, 4."
      ],
      "metadata": {
        "id": "SiGZQhq_F7mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# The average number of buses arriving in 30 minutes\n",
        "lamda = 1\n",
        "\n",
        "# The values of X (the number of buses arriving)\n",
        "x_values = [0, 1, 2, 3, 4]\n",
        "\n",
        "# Calculate the Poisson probabilities for each value of X\n",
        "poisson_probs = [math.exp(-lamda) * pow(lamda, x) / math.factorial(x) for x in x_values]\n",
        "\n",
        "# Print the probabilities\n",
        "for x, p in zip(x_values, poisson_probs):\n",
        "    print(\"P(X={}) = {:.4f}\".format(x, p))"
      ],
      "metadata": {
        "id": "tY_lB1SrF-hX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. Apply any one of the regression techniques on a dataset. Apply different evaluation metrices on the dataset"
      ],
      "metadata": {
        "id": "QjXgGr5VGCSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Load the Uber Price dataset\n",
        "df = pd.read_csv(\"https://github.com/rushiranpise-ltce/ads/raw/main/Housing.csv\")\n",
        "df.head()\n",
        "df.drop(df.columns[[5, 6, 7, 8, 9, 11, 12]], axis=1, inplace=True)\n",
        "# Split the dataset into training and testing sets\n",
        "X = df.drop(\"price\", axis=1)\n",
        "y = df[\"price\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Create a linear regression model\n",
        "model = LinearRegression()\n",
        "#drop string based columns\n",
        "# Fit the model to the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "#evaluation metrics\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "# Calculate the R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"R-squared score:\", r2)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean squared error:\", mse)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"Mean absolute error:\", mae)"
      ],
      "metadata": {
        "id": "TXU4bV_uGGQq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}