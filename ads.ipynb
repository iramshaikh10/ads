{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOt4A2NiWz1FsDgE6giYVb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rushiranpise-ltce/ads/blob/main/ads.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Find the data distributions using box and scatter plot for housing dataset"
      ],
      "metadata": {
        "id": "ZKGyfk50At8G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjaBNXOSAmOq"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"https://github.com/rushiranpise-ltce/ads/raw/main/Housing.csv\")\n",
        "\n",
        "# Show the first few rows of the dataset\n",
        "df.head()\n",
        "\n",
        "# Create a box plot of the \"price\" column\n",
        "sns.boxplot(x=df['price'])\n",
        "plt.show()\n",
        "\n",
        "# Create a box plot of the \"area\" column\n",
        "sns.boxplot(x=df['area'])\n",
        "plt.show()\n",
        "\n",
        "# Create a box plot of the \"bedrooms\" column\n",
        "sns.boxplot(x=df['bedrooms'])\n",
        "plt.show()\n",
        "\n",
        "# Create a scatter plot of \"price\" vs. \"area\"\n",
        "sns.scatterplot(x=df[\"price\"], y=df[\"area\"])\n",
        "plt.show()\n",
        "\n",
        "# Create a scatter plot of \"area\" vs. \"bedrooms\"\n",
        "sns.scatterplot(x=df[\"area\"], y=df[\"bedrooms\"])\n",
        "plt.show()\n",
        "\n",
        "# Create a scatter plot of \"price\" vs. \"bedrooms\"\n",
        "sns.scatterplot(x=df[\"price\"], y=df[\"bedrooms\"])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code imports the necessary libraries including NumPy, Pandas, Seaborn, and Matplotlib. It loads a dataset called \"Housing.csv\" using Pandas and displays the first few rows of the dataset. \n",
        "\n",
        "The code then creates box plots for the \"price\", \"area\", and \"bedrooms\" columns using Seaborn's `boxplot()` function. It also creates scatter plots for \"price\" vs. \"area\", \"area\" vs. \"bedrooms\", and \"price\" vs. \"bedrooms\" using Seaborn's `scatterplot()` function. \n",
        "\n",
        "Finally, the code displays each plot using Matplotlib's `show()` function after each plot is created."
      ],
      "metadata": {
        "id": "oWjIJufnv-le"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Find the outliers using plot. v=c(135,140,50,75,100,125,150,175,200)"
      ],
      "metadata": {
        "id": "fLTrrpf-Dmg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# The given data\n",
        "v = [135,140,50,75,100,125,150,175,200]\n",
        "\n",
        "# Plot a boxplot of the data\n",
        "plt.boxplot(v)\n",
        "\n",
        "# Find the outliers\n",
        "q1 = plt.boxplot(v)['whiskers'][0].get_ydata()[1]\n",
        "q3 = plt.boxplot(v)['whiskers'][1].get_ydata()[1]\n",
        "iqr = q3 - q1\n",
        "lower_bound = q1 - 1.5 * iqr\n",
        "upper_bound = q3 + 1.5 * iqr\n",
        "outliers = [x for x in v if x < lower_bound or x > upper_bound]\n",
        "\n",
        "# Mark the outliers in the plot\n",
        "for outlier in outliers:\n",
        "    plt.plot(v.index(outlier) + 1, outlier, 'ro')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WmMDFWugDt1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code imports the necessary library Matplotlib. It defines a list of data called \"v\" and creates a boxplot of the data using Matplotlib's `boxplot()` function.\n",
        "\n",
        "Then, the code finds the outliers in the data by calculating the lower bound and upper bound using the first and third quartiles (Q1 and Q3) and the interquartile range (IQR). The outliers are defined as any values that fall outside of the calculated bounds. \n",
        "\n",
        "Finally, the code marks the outliers on the plot by plotting a red dot above the outlier values, using the `plot()` function. The plot is then displayed using Matplotlib's `show()` function."
      ],
      "metadata": {
        "id": "SnsSfEp2wNhu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Plot the histogram, bar chart and pie chart on any sample data."
      ],
      "metadata": {
        "id": "fwBnuwliDzUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate a sample dataset of 1000 values drawn from a normal distribution with mean=10 and standard deviation=3\n",
        "data = np.random.normal(loc=10, scale=3, size=1000)\n",
        "\n",
        "# Create a histogram of the sample data using 20 bins and black edges\n",
        "plt.hist(data, bins=20, edgecolor='black')\n",
        "\n",
        "# Set the title, x-axis label, and y-axis label for the histogram\n",
        "plt.title('Histogram of Sample Data')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Display the histogram\n",
        "plt.show()\n",
        "\n",
        "# Count the number of occurrences of each integer value in the sample data\n",
        "unique, counts = np.unique(data.astype(int), return_counts=True)\n",
        "\n",
        "# Create a bar chart of the value counts\n",
        "plt.bar(unique, counts)\n",
        "\n",
        "# Set the title, x-axis label, and y-axis label for the bar chart\n",
        "plt.title('Bar Chart of Sample Data')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "# Display the bar chart\n",
        "plt.show()\n",
        "\n",
        "# Create a pie chart of the proportions of four groups\n",
        "labels = ['Group A', 'Group B', 'Group C', 'Group D']\n",
        "sizes = [15, 30, 45, 10]\n",
        "plt.pie(sizes, labels=labels, autopct='%1.1f%%')\n",
        "\n",
        "# Set the title for the pie chart\n",
        "plt.title('Pie Chart of Sample Data')\n",
        "\n",
        "# Display the pie chart\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1ejdYx0SD2yD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code imports the necessary libraries NumPy and Matplotlib. It generates a sample dataset of 1000 values drawn from a normal distribution with mean=10 and standard deviation=3 using NumPy's `random.normal()` function.\n",
        "\n",
        "The code then creates a histogram of the sample data using Matplotlib's `hist()` function with 20 bins and black edges. It sets the title, x-axis label, and y-axis label for the histogram using Matplotlib's `title()`, `xlabel()`, and `ylabel()` functions, respectively. The histogram is then displayed using Matplotlib's `show()` function.\n",
        "\n",
        "Next, the code counts the number of occurrences of each integer value in the sample data using NumPy's `unique()` function and creates a bar chart of the value counts using Matplotlib's `bar()` function. It sets the title, x-axis label, and y-axis label for the bar chart using Matplotlib's `title()`, `xlabel()`, and `ylabel()` functions, respectively. The bar chart is then displayed using Matplotlib's `show()` function.\n",
        "\n",
        "Finally, the code creates a pie chart of the proportions of four groups using Matplotlib's `pie()` function. It sets the title for the pie chart using Matplotlib's `title()` function and displays the pie chart using Matplotlib's `show()` function."
      ],
      "metadata": {
        "id": "DMiWydJgwaa1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. How to find a correlation matrix and plot the correlation on iris data set"
      ],
      "metadata": {
        "id": "lhC-K6fnD89F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the iris dataset from a URL and set the column names\n",
        "iris_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n",
        "iris_df.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
        "\n",
        "# Calculate the correlation matrix of the iris dataset\n",
        "corr_matrix = iris_df.corr()\n",
        "\n",
        "# Create a heatmap of the correlation matrix with annotations and a blue-green color scheme\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='YlGnBu')\n",
        "\n",
        "# Set the title of the heatmap\n",
        "plt.title('Correlation Matrix of Iris Dataset')\n",
        "\n",
        "# Display the heatmap\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2iaWMFjFEWZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is using the seaborn library to create a heatmap of the correlation matrix of the iris dataset. The iris dataset is loaded from a URL and the column names are set. Then, the correlation matrix of the iris dataset is calculated using pandas' corr() function. The heatmap is created using the sns.heatmap() function with annotations and a blue-green color scheme. Finally, the title of the heatmap is set and it is displayed using the plt.show() function from matplotlib."
      ],
      "metadata": {
        "id": "vgZk4CqvwglV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Outlier Distance based method. The example set used in this process is the Iris dataset with four numerical attributes and 150 examples. Find  Top five outliers of Iris dataset when  k=1  and  k =5."
      ],
      "metadata": {
        "id": "4BQJ0nNPEY3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the Iris dataset from the UCI Machine Learning Repository\n",
        "iris_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n",
        "\n",
        "# Rename the columns to the appropriate names\n",
        "iris_df.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
        "\n",
        "# Create an empty matrix to store the pairwise Euclidean distances between all instances\n",
        "distances = np.zeros((len(iris_df), len(iris_df)))\n",
        "\n",
        "# Calculate the pairwise Euclidean distances between all instances and store them in the distances matrix\n",
        "for i in range(len(iris_df)):\n",
        "    for j in range(len(iris_df)):\n",
        "        distances[i][j] = np.sqrt(np.sum((iris_df.iloc[i,:4] - iris_df.iloc[j,:4])**2))\n",
        "\n",
        "# Calculate the kth smallest distances for k=1 and k=5\n",
        "k1_distances = np.sort(distances)[:,1]\n",
        "k5_distances = np.sort(distances)[:,5]\n",
        "\n",
        "# Find the top 5 outliers with k=1 and k=5\n",
        "top5_k1 = iris_df.iloc[np.argsort(k1_distances)[-5:],:]\n",
        "top5_k5 = iris_df.iloc[np.argsort(k5_distances)[-5:],:]\n",
        "\n",
        "# Print the top 5 outliers with k=1 and k=5\n",
        "print(\"Top 5 outliers with k=1:\\n\", top5_k1)\n",
        "print(\"\\nTop 5 outliers with k=5:\\n\", top5_k5)\n"
      ],
      "metadata": {
        "id": "eSpL8S58Ees9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code is calculating the pairwise Euclidean distances between all instances in the Iris dataset and using these distances to find the top 5 outliers with k=1 and k=5. \n",
        "\n",
        "In the context of k-Nearest Neighbors (k-NN), k is the number of nearest neighbors used to classify a new data point. For example, if k=1, the classification of a new data point is based on the class of its closest neighbor. The code is using the pairwise distances to find the instances that have the largest distance to their kth nearest neighbor, which are potential outliers.\n",
        "\n",
        "The code first loads the Iris dataset from the UCI Machine Learning Repository and renames the columns to the appropriate names. Then an empty matrix is created to store the pairwise Euclidean distances between all instances. The for loops calculate the Euclidean distance between every pair of instances in the dataset using the formula `np.sqrt(np.sum((iris_df.iloc[i,:4] - iris_df.iloc[j,:4])**2))` and store it in the distances matrix. \n",
        "\n",
        "After calculating the pairwise distances, the code uses `np.sort(distances)` to sort the distances in ascending order and selects the kth smallest distance for k=1 and k=5. The outliers are then identified by selecting the instances with the largest kth smallest distance. The `np.argsort` function is used to get the indices of the sorted distances and the `iloc` function is used to select the corresponding instances from the dataset.\n",
        "\n",
        "Finally, the code prints the top 5 outliers for k=1 and k=5."
      ],
      "metadata": {
        "id": "THGUx7OzwxBF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Consider the following mice data: Height:140,142,150,147,139,152,154,135,148, 147. Weight: 59, 61, 66, 62, 57, 68, 69, 58, 63, 62. Derive relationship coefficients and summary for the above data. Find the evaluation matrices for the the data "
      ],
      "metadata": {
        "id": "eWIAqK5YEr06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the numpy library\n",
        "import numpy as np\n",
        "\n",
        "# Define the height and weight arrays\n",
        "height = [140, 142, 150, 147, 139, 152, 154, 135, 148, 147]\n",
        "weight = [59, 61, 66, 62, 57, 68, 69, 58, 63, 62]\n",
        "\n",
        "# Calculate the mean and standard deviation of the height and weight arrays\n",
        "mean_height = np.mean(height)\n",
        "mean_weight = np.mean(weight)\n",
        "std_height = np.std(height)\n",
        "std_weight = np.std(weight)\n",
        "\n",
        "# Print the results to the console\n",
        "print(\"Mean height:\", mean_height)\n",
        "print(\"Mean weight:\", mean_weight)\n",
        "print(\"Standard deviation of height:\", std_height)\n",
        "print(\"Standard deviation of weight:\", std_weight)\n",
        "\n",
        "# Calculate the covariance and Pearson correlation coefficient of the height and weight arrays\n",
        "covariance = np.cov(height, weight)[0][1]\n",
        "print(\"Covariance:\", covariance)\n",
        "corr_coef = covariance / (std_height * std_weight)\n",
        "print(\"Pearson correlation coefficient:\", corr_coef)\n",
        "\n",
        "# Calculate the coefficient of determination (r-squared)\n",
        "r_squared = corr_coef**2\n",
        "print(\"Coefficient of determination (r-squared):\", r_squared)\n",
        "\n",
        "# Import the matplotlib.pyplot library for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a scatter plot of the height and weight data\n",
        "plt.scatter(height, weight)\n",
        "plt.xlabel(\"Height\")\n",
        "plt.ylabel(\"Weight\")\n",
        "plt.show()\n",
        "\n",
        "# Import the necessary libraries\n",
        "from pandas import read_csv\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# Load the airline passengers dataset from GitHub\n",
        "series = read_csv('https://github.com/rushiranpise-ltce/ads/raw/main/airline-passengers.csv', header=0, index_col=0)\n",
        "\n",
        "# Plot the raw time series data\n",
        "series.plot()\n",
        "pyplot.show()\n",
        "\n",
        "# Perform seasonal decomposition and plot the results\n",
        "result = seasonal_decompose(series, model='multiplicative', period=1)\n",
        "result.plot()\n",
        "pyplot.show()\n"
      ],
      "metadata": {
        "id": "Y2odp3jOE6DP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code is calculating various statistical measures such as mean, standard deviation, covariance, correlation coefficient, and coefficient of determination for two sets of data (height and weight). It then creates a scatter plot of the data using the matplotlib library. \n",
        "\n",
        "It also loads a time series dataset of airline passengers from a GitHub repository using the pandas library and then performs seasonal decomposition of the time series data using the statsmodels library and plots the results using matplotlib."
      ],
      "metadata": {
        "id": "d-b_8SK-xQxF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Write a python code to decompose time series data into random, trend and seasonal data."
      ],
      "metadata": {
        "id": "2WL4uyzwE7fS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "from pandas import read_csv\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# Load the airline passengers dataset from GitHub\n",
        "series = read_csv('https://github.com/rushiranpise-ltce/ads/raw/main/airline-passengers.csv', header=0, index_col=0)\n",
        "\n",
        "# Plot the raw time series data\n",
        "series.plot()\n",
        "pyplot.show()\n",
        "\n",
        "# Perform seasonal decomposition and plot the results\n",
        "result = seasonal_decompose(series, model='multiplicative', period=1)\n",
        "result.plot()\n",
        "pyplot.show()\n"
      ],
      "metadata": {
        "id": "HqBpHNykE_Ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code imports necessary libraries such as pandas, statsmodels, and matplotlib.pyplot. It then loads a time series data of airline passengers from a Github repository using the read_csv function of pandas. The raw time series data is then plotted using the plot() function of the pandas dataframe. The code then uses the seasonal_decompose function of the statsmodels library to decompose the time series into its seasonal, trend, and residual components. Finally, it plots the decomposed components using the plot() function of the resulting object."
      ],
      "metadata": {
        "id": "x_jA4xq7xXJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. write a python program to calculate the variance for :\n",
        "    sample1 = (1, 2, 5, 4, 8, 9, 12)\n",
        "    sample2 = (-2, -4, -3, -1, -5, -6)\n",
        "    sample3 = (-9, -1, -0, 2, 1, 3, 4, 19)\n",
        "    sample5 = (1.23, 1.45, 2.1, 2.2, 1.9)\n"
      ],
      "metadata": {
        "id": "GiVJllleFfMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing NumPy library\n",
        "import numpy as np\n",
        "\n",
        "# Defining sample data\n",
        "sample1 = (1, 2, 5, 4, 8, 9, 12)\n",
        "sample2 = (-2, -4, -3, -1, -5, -6)\n",
        "sample3 = (-9, -1, 0, 2, 1, 3, 4, 19)\n",
        "sample4 = (1.23, 1.45, 2.1, 2.2, 1.9)\n",
        "\n",
        "# Computing the sample variances using np.var() function\n",
        "# The \"ddof\" parameter is set to 1 to calculate the unbiased estimator of variance\n",
        "variance1 = np.var(sample1, ddof=1)\n",
        "variance2 = np.var(sample2, ddof=1)\n",
        "variance3 = np.var(sample3, ddof=1)\n",
        "variance4 = np.var(sample4, ddof=1)\n",
        "\n",
        "# Printing the computed variances with two decimal places\n",
        "print(\"Variance of sample1: {:.2f}\".format(variance1))\n",
        "print(\"Variance of sample2: {:.2f}\".format(variance2))\n",
        "print(\"Variance of sample3: {:.2f}\".format(variance3))\n",
        "print(\"Variance of sample4: {:.2f}\".format(variance4))\n"
      ],
      "metadata": {
        "id": "0js1t5ZZFkQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code demonstrates how to use the NumPy library to calculate the sample variance for a given set of data. \n",
        "\n",
        "Four different sets of sample data are defined in the code. The np.var() function is then used to calculate the variance of each sample, with the \"ddof\" parameter set to 1 to calculate the unbiased estimator of variance. Finally, the computed variances are printed with two decimal places.\n",
        "\n",
        "Note that variance is a measure of the spread or dispersion of a dataset, and it is calculated by finding the average of the squared differences from the mean. The unbiased estimator of variance is used when the sample size is small and the sample data may not accurately represent the entire population."
      ],
      "metadata": {
        "id": "EtvJmDHGxe8F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. How do you find outliers in Uber fares dataset using visualization techniques?"
      ],
      "metadata": {
        "id": "GyBXFPYPFosY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reading in the data from a CSV file on GitHub\n",
        "df = pd.read_csv('https://github.com/rushiranpise-ltce/ads/raw/main/uber.csv')\n",
        "\n",
        "# Creating a box plot of the fare_amount variable\n",
        "plt.boxplot(df['fare_amount'])\n",
        "plt.show()\n",
        "\n",
        "# Creating a histogram of the fare_amount variable with 30 bins\n",
        "plt.hist(df['fare_amount'], bins=30)\n",
        "plt.show()\n",
        "\n",
        "# Creating a scatter plot of the passenger_count and fare_amount variables\n",
        "plt.scatter(df['passenger_count'], df['fare_amount'])\n",
        "plt.xlabel('passenger_count')\n",
        "plt.ylabel('fare_amount')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NqNKwryyFu2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code reads in data from a CSV file hosted on GitHub into a Pandas DataFrame. Then, it creates visualizations of the fare amount data using matplotlib. The first plot is a box plot of the fare_amount variable to display the distribution of the data and any outliers. The second plot is a histogram of the fare_amount variable with 30 bins to display the distribution of the data. The third plot is a scatter plot of the passenger_count and fare_amount variables to explore the relationship between the two variables. The x-axis represents the passenger count and the y-axis represents the fare amount."
      ],
      "metadata": {
        "id": "kzDpdloaxoBE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Finding outliers using statistical methods for any dataset ?"
      ],
      "metadata": {
        "id": "MIMSbUwJF1i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing NumPy library\n",
        "import numpy as np\n",
        "\n",
        "# Creating an array of data\n",
        "data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 100])\n",
        "\n",
        "# Computing the first and third quartiles of the data using np.percentile() function\n",
        "q1, q3 = np.percentile(data, [25, 75])\n",
        "\n",
        "# Computing the interquartile range (IQR)\n",
        "iqr = q3 - q1\n",
        "\n",
        "# Setting the threshold for outlier detection\n",
        "threshold = 1.5\n",
        "\n",
        "# Calculating the lower and upper bounds for outlier detection\n",
        "lower_bound = q1 - threshold * iqr\n",
        "upper_bound = q3 + threshold * iqr\n",
        "\n",
        "# Detecting outliers by identifying values that are outside the lower and upper bounds\n",
        "outliers = np.where((data < lower_bound) | (data > upper_bound))\n",
        "\n",
        "# Printing the detected outliers\n",
        "print(\"Outliers:\", data[outliers])\n"
      ],
      "metadata": {
        "id": "slkpUf8UF4HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code uses NumPy library to detect outliers in a given array of data. The steps are:\n",
        "\n",
        "1. A data array is created with 11 elements including some outliers.\n",
        "2. The first and third quartiles (Q1 and Q3) of the data are calculated using np.percentile() function.\n",
        "3. The interquartile range (IQR) is computed as Q3 - Q1.\n",
        "4. A threshold value of 1.5 is set to detect outliers.\n",
        "5. Lower and upper bounds for outlier detection are computed as Q1 - threshold*IQR and Q3 + threshold*IQR, respectively.\n",
        "6. Using the np.where() function, outliers are detected by identifying values that are outside the lower and upper bounds.\n",
        "7. Finally, the detected outliers are printed.\n",
        "\n",
        "Note: This is a simple example to illustrate outlier detection using IQR. In practice, various other techniques can be used based on the nature of data and the problem at hand."
      ],
      "metadata": {
        "id": "QUzrdA6OxvsF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. The number of buses that come on a bus stop in span of 30 minutes is 1.Write a program in to represent  Poisson distribution  to be used to model the probability of different number of buses, X, coming to the bus stop within the next 30 minutes where X can take value of 0, 1, 2, 3, 4."
      ],
      "metadata": {
        "id": "SiGZQhq_F7mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the math library\n",
        "import math\n",
        "\n",
        "# Setting the Poisson distribution parameter\n",
        "lamda = 1\n",
        "\n",
        "# Creating a list of x values\n",
        "x_values = [0, 1, 2, 3, 4]\n",
        "\n",
        "# Computing the Poisson probabilities for each x value using a list comprehension\n",
        "poisson_probs = [math.exp(-lamda) * pow(lamda, x) / math.factorial(x) for x in x_values]\n",
        "\n",
        "# Looping through each x value and its corresponding Poisson probability and printing the results\n",
        "for x, p in zip(x_values, poisson_probs):\n",
        "    print(\"P(X={}) = {:.4f}\".format(x, p))\n"
      ],
      "metadata": {
        "id": "tY_lB1SrF-hX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code calculates and prints the probabilities of a Poisson distribution with a parameter lambda equal to 1, for the values of x equal to 0, 1, 2, 3, and 4. It uses the math library to compute the exponential and factorial functions. The probabilities are computed using the formula P(X=x) = e^(-lambda) * lambda^x / x!, where lambda is the Poisson distribution parameter and x is the number of occurrences. The output shows each x value and its corresponding Poisson probability with four decimal places."
      ],
      "metadata": {
        "id": "5IXcUHx-yWRd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. Apply any one of the regression techniques on a dataset. Apply different evaluation metrices on the dataset"
      ],
      "metadata": {
        "id": "QjXgGr5VGCSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Reading in the housing dataset from a CSV file hosted on GitHub\n",
        "df = pd.read_csv(\"https://github.com/rushiranpise-ltce/ads/raw/main/Housing.csv\")\n",
        "\n",
        "# Dropping unnecessary columns from the dataset\n",
        "df.drop(df.columns[[5, 6, 7, 8, 9, 11, 12]], axis=1, inplace=True)\n",
        "\n",
        "# Setting the predictor variables (X) and the target variable (y)\n",
        "X = df.drop(\"price\", axis=1)\n",
        "y = df[\"price\"]\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Creating a Linear Regression model and fitting it to the training data\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions on the testing data and computing various evaluation metrics\n",
        "y_pred = model.predict(X_test)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"R-squared score:\", r2)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean squared error:\", mse)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"Mean absolute error:\", mae)\n"
      ],
      "metadata": {
        "id": "TXU4bV_uGGQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code reads in the housing dataset from a CSV file hosted on GitHub, drops unnecessary columns from the dataset, sets the predictor variables (X) and the target variable (y), splits the data into training and testing sets using `train_test_split` function from `sklearn.model_selection`, creates a Linear Regression model and fits it to the training data, makes predictions on the testing data, and computes various evaluation metrics such as R-squared score, mean squared error, and mean absolute error using `r2_score`, `mean_squared_error`, and `mean_absolute_error` functions from `sklearn.metrics`.\n",
        "\n",
        "Overall, the code performs a basic machine learning task of building a linear regression model to predict the price of a house based on its features, and evaluating the performance of the model using various metrics."
      ],
      "metadata": {
        "id": "3LkE6AF7zGVc"
      }
    }
  ]
}