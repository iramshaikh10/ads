{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQeEeMwx21PzDMqUvYeRpp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rushiranpise-ltce/ads/blob/main/ads.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Find the data distributions using box and scatter plot for housing dataset"
      ],
      "metadata": {
        "id": "ZKGyfk50At8G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjaBNXOSAmOq"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"https://github.com/rushiranpise-ltce/ads/raw/main/Housing.csv\")\n",
        "\n",
        "# Show the first few rows of the dataset\n",
        "df.head()\n",
        "\n",
        "# Create a box plot of the \"price\" column\n",
        "sns.boxplot(x=df['price'])\n",
        "plt.show()\n",
        "\n",
        "# Create a box plot of the \"area\" column\n",
        "sns.boxplot(x=df['area'])\n",
        "plt.show()\n",
        "\n",
        "# Create a box plot of the \"bedrooms\" column\n",
        "sns.boxplot(x=df['bedrooms'])\n",
        "plt.show()\n",
        "\n",
        "# Create a scatter plot of \"price\" vs. \"area\"\n",
        "sns.scatterplot(x=df[\"price\"], y=df[\"area\"])\n",
        "plt.show()\n",
        "\n",
        "# Create a scatter plot of \"area\" vs. \"bedrooms\"\n",
        "sns.scatterplot(x=df[\"area\"], y=df[\"bedrooms\"])\n",
        "plt.show()\n",
        "\n",
        "# Create a scatter plot of \"price\" vs. \"bedrooms\"\n",
        "sns.scatterplot(x=df[\"price\"], y=df[\"bedrooms\"])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Find the outliers using plot. v=c(135,140,50,75,100,125,150,175,200)"
      ],
      "metadata": {
        "id": "fLTrrpf-Dmg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# The given data\n",
        "v = [135,140,50,75,100,125,150,175,200]\n",
        "\n",
        "# Plot a boxplot of the data\n",
        "plt.boxplot(v)\n",
        "\n",
        "# Find the outliers\n",
        "q1 = plt.boxplot(v)['whiskers'][0].get_ydata()[1]\n",
        "q3 = plt.boxplot(v)['whiskers'][1].get_ydata()[1]\n",
        "iqr = q3 - q1\n",
        "lower_bound = q1 - 1.5 * iqr\n",
        "upper_bound = q3 + 1.5 * iqr\n",
        "outliers = [x for x in v if x < lower_bound or x > upper_bound]\n",
        "\n",
        "# Mark the outliers in the plot\n",
        "for outlier in outliers:\n",
        "    plt.plot(v.index(outlier) + 1, outlier, 'ro')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WmMDFWugDt1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Plot the histogram, bar chart and pie chart on any sample data."
      ],
      "metadata": {
        "id": "fwBnuwliDzUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate a sample dataset of 1000 values drawn from a normal distribution with mean=10 and standard deviation=3\n",
        "data = np.random.normal(loc=10, scale=3, size=1000)\n",
        "\n",
        "# Create a histogram of the sample data using 20 bins and black edges\n",
        "plt.hist(data, bins=20, edgecolor='black')\n",
        "\n",
        "# Set the title, x-axis label, and y-axis label for the histogram\n",
        "plt.title('Histogram of Sample Data')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Display the histogram\n",
        "plt.show()\n",
        "\n",
        "# Count the number of occurrences of each integer value in the sample data\n",
        "unique, counts = np.unique(data.astype(int), return_counts=True)\n",
        "\n",
        "# Create a bar chart of the value counts\n",
        "plt.bar(unique, counts)\n",
        "\n",
        "# Set the title, x-axis label, and y-axis label for the bar chart\n",
        "plt.title('Bar Chart of Sample Data')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "# Display the bar chart\n",
        "plt.show()\n",
        "\n",
        "# Create a pie chart of the proportions of four groups\n",
        "labels = ['Group A', 'Group B', 'Group C', 'Group D']\n",
        "sizes = [15, 30, 45, 10]\n",
        "plt.pie(sizes, labels=labels, autopct='%1.1f%%')\n",
        "\n",
        "# Set the title for the pie chart\n",
        "plt.title('Pie Chart of Sample Data')\n",
        "\n",
        "# Display the pie chart\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1ejdYx0SD2yD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. How to find a correlation matrix and plot the correlation on iris data set"
      ],
      "metadata": {
        "id": "lhC-K6fnD89F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the iris dataset from a URL and set the column names\n",
        "iris_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n",
        "iris_df.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
        "\n",
        "# Calculate the correlation matrix of the iris dataset\n",
        "corr_matrix = iris_df.corr()\n",
        "\n",
        "# Create a heatmap of the correlation matrix with annotations and a blue-green color scheme\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='YlGnBu')\n",
        "\n",
        "# Set the title of the heatmap\n",
        "plt.title('Correlation Matrix of Iris Dataset')\n",
        "\n",
        "# Display the heatmap\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2iaWMFjFEWZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Outlier Distance based method. The example set used in this process is the Iris dataset with four numerical attributes and 150 examples. Find  Top five outliers of Iris dataset when  k=1  and  k =5."
      ],
      "metadata": {
        "id": "4BQJ0nNPEY3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the Iris dataset from the UCI Machine Learning Repository\n",
        "iris_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n",
        "\n",
        "# Rename the columns to the appropriate names\n",
        "iris_df.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
        "\n",
        "# Create an empty matrix to store the pairwise Euclidean distances between all instances\n",
        "distances = np.zeros((len(iris_df), len(iris_df)))\n",
        "\n",
        "# Calculate the pairwise Euclidean distances between all instances and store them in the distances matrix\n",
        "for i in range(len(iris_df)):\n",
        "    for j in range(len(iris_df)):\n",
        "        distances[i][j] = np.sqrt(np.sum((iris_df.iloc[i,:4] - iris_df.iloc[j,:4])**2))\n",
        "\n",
        "# Calculate the kth smallest distances for k=1 and k=5\n",
        "k1_distances = np.sort(distances)[:,1]\n",
        "k5_distances = np.sort(distances)[:,5]\n",
        "\n",
        "# Find the top 5 outliers with k=1 and k=5\n",
        "top5_k1 = iris_df.iloc[np.argsort(k1_distances)[-5:],:]\n",
        "top5_k5 = iris_df.iloc[np.argsort(k5_distances)[-5:],:]\n",
        "\n",
        "# Print the top 5 outliers with k=1 and k=5\n",
        "print(\"Top 5 outliers with k=1:\\n\", top5_k1)\n",
        "print(\"\\nTop 5 outliers with k=5:\\n\", top5_k5)\n"
      ],
      "metadata": {
        "id": "eSpL8S58Ees9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Consider the following mice data: Height:140,142,150,147,139,152,154,135,148, 147. Weight: 59, 61, 66, 62, 57, 68, 69, 58, 63, 62. Derive relationship coefficients and summary for the above data. Find the evaluation matrices for the the data "
      ],
      "metadata": {
        "id": "eWIAqK5YEr06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the numpy library\n",
        "import numpy as np\n",
        "\n",
        "# Define the height and weight arrays\n",
        "height = [140, 142, 150, 147, 139, 152, 154, 135, 148, 147]\n",
        "weight = [59, 61, 66, 62, 57, 68, 69, 58, 63, 62]\n",
        "\n",
        "# Calculate the mean and standard deviation of the height and weight arrays\n",
        "mean_height = np.mean(height)\n",
        "mean_weight = np.mean(weight)\n",
        "std_height = np.std(height)\n",
        "std_weight = np.std(weight)\n",
        "\n",
        "# Print the results to the console\n",
        "print(\"Mean height:\", mean_height)\n",
        "print(\"Mean weight:\", mean_weight)\n",
        "print(\"Standard deviation of height:\", std_height)\n",
        "print(\"Standard deviation of weight:\", std_weight)\n",
        "\n",
        "# Calculate the covariance and Pearson correlation coefficient of the height and weight arrays\n",
        "covariance = np.cov(height, weight)[0][1]\n",
        "print(\"Covariance:\", covariance)\n",
        "corr_coef = covariance / (std_height * std_weight)\n",
        "print(\"Pearson correlation coefficient:\", corr_coef)\n",
        "\n",
        "# Calculate the coefficient of determination (r-squared)\n",
        "r_squared = corr_coef**2\n",
        "print(\"Coefficient of determination (r-squared):\", r_squared)\n",
        "\n",
        "# Import the matplotlib.pyplot library for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a scatter plot of the height and weight data\n",
        "plt.scatter(height, weight)\n",
        "plt.xlabel(\"Height\")\n",
        "plt.ylabel(\"Weight\")\n",
        "plt.show()\n",
        "\n",
        "# Import the necessary libraries\n",
        "from pandas import read_csv\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# Load the airline passengers dataset from GitHub\n",
        "series = read_csv('https://github.com/rushiranpise-ltce/ads/raw/main/airline-passengers.csv', header=0, index_col=0)\n",
        "\n",
        "# Plot the raw time series data\n",
        "series.plot()\n",
        "pyplot.show()\n",
        "\n",
        "# Perform seasonal decomposition and plot the results\n",
        "result = seasonal_decompose(series, model='multiplicative', period=1)\n",
        "result.plot()\n",
        "pyplot.show()\n"
      ],
      "metadata": {
        "id": "Y2odp3jOE6DP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Write a python code to decompose time series data into random, trend and seasonal data."
      ],
      "metadata": {
        "id": "2WL4uyzwE7fS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "from pandas import read_csv\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# Load the airline passengers dataset from GitHub\n",
        "series = read_csv('https://github.com/rushiranpise-ltce/ads/raw/main/airline-passengers.csv', header=0, index_col=0)\n",
        "\n",
        "# Plot the raw time series data\n",
        "series.plot()\n",
        "pyplot.show()\n",
        "\n",
        "# Perform seasonal decomposition and plot the results\n",
        "result = seasonal_decompose(series, model='multiplicative', period=1)\n",
        "result.plot()\n",
        "pyplot.show()\n"
      ],
      "metadata": {
        "id": "HqBpHNykE_Ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. write a python program to calculate the variance for :\n",
        "    sample1 = (1, 2, 5, 4, 8, 9, 12)\n",
        "    sample2 = (-2, -4, -3, -1, -5, -6)\n",
        "    sample3 = (-9, -1, -0, 2, 1, 3, 4, 19)\n",
        "    sample5 = (1.23, 1.45, 2.1, 2.2, 1.9)\n"
      ],
      "metadata": {
        "id": "GiVJllleFfMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing NumPy library\n",
        "import numpy as np\n",
        "\n",
        "# Defining sample data\n",
        "sample1 = (1, 2, 5, 4, 8, 9, 12)\n",
        "sample2 = (-2, -4, -3, -1, -5, -6)\n",
        "sample3 = (-9, -1, 0, 2, 1, 3, 4, 19)\n",
        "sample4 = (1.23, 1.45, 2.1, 2.2, 1.9)\n",
        "\n",
        "# Computing the sample variances using np.var() function\n",
        "# The \"ddof\" parameter is set to 1 to calculate the unbiased estimator of variance\n",
        "variance1 = np.var(sample1, ddof=1)\n",
        "variance2 = np.var(sample2, ddof=1)\n",
        "variance3 = np.var(sample3, ddof=1)\n",
        "variance4 = np.var(sample4, ddof=1)\n",
        "\n",
        "# Printing the computed variances with two decimal places\n",
        "print(\"Variance of sample1: {:.2f}\".format(variance1))\n",
        "print(\"Variance of sample2: {:.2f}\".format(variance2))\n",
        "print(\"Variance of sample3: {:.2f}\".format(variance3))\n",
        "print(\"Variance of sample4: {:.2f}\".format(variance4))\n"
      ],
      "metadata": {
        "id": "0js1t5ZZFkQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. How do you find outliers in Uber fares dataset using visualization techniques?"
      ],
      "metadata": {
        "id": "GyBXFPYPFosY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reading in the data from a CSV file on GitHub\n",
        "df = pd.read_csv('https://github.com/rushiranpise-ltce/ads/raw/main/uber.csv')\n",
        "\n",
        "# Creating a box plot of the fare_amount variable\n",
        "plt.boxplot(df['fare_amount'])\n",
        "plt.show()\n",
        "\n",
        "# Creating a histogram of the fare_amount variable with 30 bins\n",
        "plt.hist(df['fare_amount'], bins=30)\n",
        "plt.show()\n",
        "\n",
        "# Creating a scatter plot of the passenger_count and fare_amount variables\n",
        "plt.scatter(df['passenger_count'], df['fare_amount'])\n",
        "plt.xlabel('passenger_count')\n",
        "plt.ylabel('fare_amount')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NqNKwryyFu2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Finding outliers using statistical methods for any dataset ?"
      ],
      "metadata": {
        "id": "MIMSbUwJF1i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing NumPy library\n",
        "import numpy as np\n",
        "\n",
        "# Creating an array of data\n",
        "data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 100])\n",
        "\n",
        "# Computing the first and third quartiles of the data using np.percentile() function\n",
        "q1, q3 = np.percentile(data, [25, 75])\n",
        "\n",
        "# Computing the interquartile range (IQR)\n",
        "iqr = q3 - q1\n",
        "\n",
        "# Setting the threshold for outlier detection\n",
        "threshold = 1.5\n",
        "\n",
        "# Calculating the lower and upper bounds for outlier detection\n",
        "lower_bound = q1 - threshold * iqr\n",
        "upper_bound = q3 + threshold * iqr\n",
        "\n",
        "# Detecting outliers by identifying values that are outside the lower and upper bounds\n",
        "outliers = np.where((data < lower_bound) | (data > upper_bound))\n",
        "\n",
        "# Printing the detected outliers\n",
        "print(\"Outliers:\", data[outliers])\n"
      ],
      "metadata": {
        "id": "slkpUf8UF4HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. The number of buses that come on a bus stop in span of 30 minutes is 1.Write a program in to represent  Poisson distribution  to be used to model the probability of different number of buses, X, coming to the bus stop within the next 30 minutes where X can take value of 0, 1, 2, 3, 4."
      ],
      "metadata": {
        "id": "SiGZQhq_F7mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the math library\n",
        "import math\n",
        "\n",
        "# Setting the Poisson distribution parameter\n",
        "lamda = 1\n",
        "\n",
        "# Creating a list of x values\n",
        "x_values = [0, 1, 2, 3, 4]\n",
        "\n",
        "# Computing the Poisson probabilities for each x value using a list comprehension\n",
        "poisson_probs = [math.exp(-lamda) * pow(lamda, x) / math.factorial(x) for x in x_values]\n",
        "\n",
        "# Looping through each x value and its corresponding Poisson probability and printing the results\n",
        "for x, p in zip(x_values, poisson_probs):\n",
        "    print(\"P(X={}) = {:.4f}\".format(x, p))\n"
      ],
      "metadata": {
        "id": "tY_lB1SrF-hX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. Apply any one of the regression techniques on a dataset. Apply different evaluation metrices on the dataset"
      ],
      "metadata": {
        "id": "QjXgGr5VGCSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Reading in the housing dataset from a CSV file hosted on GitHub\n",
        "df = pd.read_csv(\"https://github.com/rushiranpise-ltce/ads/raw/main/Housing.csv\")\n",
        "\n",
        "# Dropping unnecessary columns from the dataset\n",
        "df.drop(df.columns[[5, 6, 7, 8, 9, 11, 12]], axis=1, inplace=True)\n",
        "\n",
        "# Setting the predictor variables (X) and the target variable (y)\n",
        "X = df.drop(\"price\", axis=1)\n",
        "y = df[\"price\"]\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Creating a Linear Regression model and fitting it to the training data\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions on the testing data and computing various evaluation metrics\n",
        "y_pred = model.predict(X_test)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"R-squared score:\", r2)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean squared error:\", mse)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"Mean absolute error:\", mae)\n"
      ],
      "metadata": {
        "id": "TXU4bV_uGGQq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}